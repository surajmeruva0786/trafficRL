# Traffic Signal Optimization Configuration

# SUMO Environment Settings
sumo:
  gui: true  # Set to true to visualize during training
  step_length: 1  # Simulation step length in seconds
  yellow_time: 3  # Yellow phase duration in seconds
  min_green_time: 10  # Minimum green phase duration
  max_green_time: 60  # Maximum green phase duration
  network_file: "traffic_rl/sumo/network.net.xml"
  route_file: "traffic_rl/sumo/routes.rou.xml"
  config_file: "traffic_rl/sumo/simulation.sumocfg"

# Traffic Generation
traffic:
  episode_duration: 3600  # Episode duration in seconds
  num_vehicles_per_episode: 1000  # Reduced from 1000 to prevent gridlock
  # Traffic distribution: balanced, ns_heavy, ew_heavy
  distribution: "balanced"
  # Probabilities for each direction (N, S, E, W)
  balanced: [0.25, 0.25, 0.25, 0.25]
  ns_heavy: [0.35, 0.35, 0.15, 0.15]
  ew_heavy: [0.15, 0.15, 0.35, 0.35]

# DQN Agent Hyperparameters
dqn:
  # Network architecture
  hidden_layers: [128, 128]  # Hidden layer sizes
  
  # Training parameters
  learning_rate: 0.0001
  gamma: 0.99  # Discount factor
  batch_size: 64
  
  # Replay buffer
  buffer_size: 50000
  min_buffer_size: 1000  # Start training after this many transitions
  
  # Exploration
  epsilon_start: 1.0
  epsilon_end: 0.05
  epsilon_decay_steps: 500000  # Linear decay over this many steps (~140 episodes)
  
  # Target network
  target_update_frequency: 1000  # Update target network every N steps
  
  # Training
  max_episodes: 200  # Match baseline DQN for fair comparison, ensure head specialization
  max_steps_per_episode: 3600  # Should match episode_duration
  
  # Checkpointing
  save_frequency: 10  # Save model every N episodes
  model_save_path: "models/dqn_best.pth"

# Reward Function Weights
reward:
  waiting_time_weight: -0.1   # Penalty for normalized waiting time (reduced from -1.0)
  queue_length_weight: -0.1   # Penalty for normalized queue fraction (reduced from -1.0)
  throughput_weight: 1.0      # Reward for each completed vehicle (increased from 0.1)
  phase_change_penalty: -0.05 # Small penalty to avoid excessive switching (reduced from -0.1)

# Evaluation
evaluation:
  num_episodes: 10
  epsilon: 0.0  # Greedy policy for evaluation

# Logging
logging:
  log_dir: "logs"
  results_dir: "results"
  log_frequency: 1  # Log every N episodes
  tensorboard: true

# Fixed-time Baseline
baseline:
  ns_green_time: 30
  ew_green_time: 30
  yellow_time: 5

# Multi-Head DQN Configuration
multihead_dqn:
  # Architecture
  num_heads: 3  # One per regime (low/medium/high)
  encoder_layers: [256, 256]  # Shared encoder hidden layers
  head_layers: [128]  # Individual Q-head hidden layers
  classifier_layers: [128, 64]  # Regime classifier hidden layers
  
  # Gating mechanism
  gating_type: "soft"  # "hard" (select one head) or "soft" (weighted mixture)
  
  # Training
  classifier_loss_weight: 0.1  # Weight for classifier loss in total loss
  regime_buffer_separate: false  # Use separate replay buffers per regime (future feature)
  
  # Regime thresholds for classification (adjusted for actual traffic levels)
  # Based on observed metrics: ~0.40 queue/lane (~14 total for 36 lanes), ~5s wait
  regime_thresholds:
    low_queue_max: 3      # Max total queue length for low traffic (very light)
    low_wait_max: 10      # Max avg waiting time for low traffic (very light)
    med_queue_max: 8      # Max total queue length for medium traffic (moderate)
    med_wait_max: 25      # Max avg waiting time for medium traffic (moderate)
    # High traffic: > 8 queue or > 25s wait
  
  # Specialization tracking
  track_head_specialization: true
  specialization_log_frequency: 10  # Log specialization every N episodes

